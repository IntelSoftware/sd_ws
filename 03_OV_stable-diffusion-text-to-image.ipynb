{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation with Stable Diffusion and OpenVINO‚Ñ¢\n",
    "\n",
    "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/). It is trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database. This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder.\n",
    "See the [model card](https://huggingface.co/CompVis/stable-diffusion) for more information.\n",
    "\n",
    "General diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image.\n",
    "Diffusion models have shown to achieve state-of-the-art results for generating image data. But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to train these models and also use them for inference. OpenVINO brings capabilities to run model inference on Intel hardware and opens the door to the fantastic world of diffusion models for everyone!\n",
    "\n",
    "Model capabilities are not limited text-to-image only, it also is able solve additional tasks, for example text-guided image-to-image generation and inpainting. This tutorial also considers how to run text-guided image-to-image generation using Stable Diffusion.\n",
    "\n",
    "\n",
    "This notebook demonstrates how to convert and run stable diffusion model using OpenVINO."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Prepare Inference Pipelines](#Prepare-Inference-Pipelines)\n",
    "- [Text-to-image pipeline](#Text-to-image-pipeline)\n",
    "    - [Load Stable Diffusion model and create text-to-image pipeline](#Load-Stable-Diffusion-model-and-create-text-to-image-pipeline)\n",
    "    - [Text-to-Image generation](#Text-to-Image-generation)\n",
    "    - [Interactive text-to-image demo](#Interactive-text-to-image-demo)\n",
    "- [Image-to-Image pipeline](#Image-to-Image-pipeline)\n",
    "    - [Create image-to-Image pipeline](#Create-image-to-Image-pipeline)\n",
    "    - [Image-to-Image generation](#Image-to-Image-generation)\n",
    "    - [Interactive image-to-image demo](#Interactive-image-to-image-demo)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2023.1.0\" \"git+https://github.com/huggingface/optimum-intel.git\"\n",
    "#%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu \"diffusers>=0.9.0\" \"torch>=2.1\"\n",
    "%pip install -q \"huggingface-hub>=0.9.1\"\n",
    "%pip install -q \"gradio>=4.19\"\n",
    "%pip install -q transformers Pillow opencv-python tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This adds the .local/bin to the execution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import site\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Inference Pipelines\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Let us now take a closer look at how the model works in inference by illustrating the logical flow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sd-pipeline](https://user-images.githubusercontent.com/29454499/260981188-c112dd0a-5752-4515-adca-8b09bea5d14a.png)\n",
    "\n",
    "As you can see from the diagram, the only difference between Text-to-Image and text-guided Image-to-Image generation in approach is how initial latent state is generated. In case of Image-to-Image generation, you additionally have an image encoded by VAE encoder mixed with the noise produced by using latent seed, while in Text-to-Image you use only noise as initial latent state.\n",
    "The stable diffusion model takes both a latent image representation of size $64 \\times 64$ and a text prompt is transformed to text embeddings of size $77 \\times 768$ via CLIP's text encoder as an input.\n",
    "\n",
    "Next, the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. Many different scheduler algorithms can be used for this computation, each having its pros and cons. For Stable Diffusion, it is recommended to use one of:\n",
    "\n",
    "- [PNDM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_pndm.py)\n",
    "- [DDIM scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py)\n",
    "- [K-LMS scheduler](https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_lms_discrete.py)(you will use it in your pipeline)\n",
    "\n",
    "Theory on how the scheduler algorithm function works is out of scope for this notebook. Nonetheless, in short, you should remember that you compute the predicted denoised image representation from the previous noise representation and the predicted noise residual.\n",
    "For more information, refer to the recommended [Elucidating the Design Space of Diffusion-Based Generative Models](https://arxiv.org/abs/2206.00364)\n",
    "\n",
    "The *denoising* process is repeated given number of times (by default 50) to step-by-step retrieve better latent image representations.\n",
    "When complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-image pipeline\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "\n",
    "### Load Stable Diffusion model and create text-to-image pipeline\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "We will load optimized Stable Diffusion model from the Hugging Face Hub and create pipeline to run an inference with OpenVINO Runtime by [Optimum Intel](https://huggingface.co/docs/optimum/intel/inference#stable-diffusion). \n",
    "\n",
    "For running the Stable Diffusion model with Optimum Intel, we will use the `optimum.intel.OVStableDiffusionPipeline` class, which represents the inference pipeline. `OVStableDiffusionPipeline` initialized by the `from_pretrained` method. It supports on-the-fly conversion models from PyTorch using the `export=True` parameter. A converted model can be saved on disk using the `save_pretrained` method for the next running.\n",
    "\n",
    "When Stable Diffusion models are exported to the OpenVINO format, they are decomposed into three components that consist of four models combined during inference into the pipeline:\n",
    "\n",
    "* The text encoder\n",
    "    * The text-encoder is responsible for transforming the input prompt(for example \"a photo of an astronaut riding a horse\") into an embedding space that can be understood by the U-Net. It is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text embeddings.\n",
    "* The U-NET\n",
    "    * Model predicts the `sample` state for the next step.\n",
    "* The VAE encoder\n",
    "    * The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the U-Net model.\n",
    "* The VAE decoder\n",
    "    * The decoder transforms the latent representation back into an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On the ITDC run sycl-ls from a terminal to determine which PVC you were assigned\n",
    "\n",
    "-- ONEAPI_DEVICE_SELECTOR environment variable is set to opencl:*;level_zero:4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVStableDiffusionPipeline\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = device.value\n",
    "\n",
    "MODEL_ID = \"prompthero/openjourney\"\n",
    "MODEL_DIR = Path(\"diffusion_pipeline\")\n",
    "\n",
    "if not MODEL_DIR.exists():\n",
    "    ov_pipe = OVStableDiffusionPipeline.from_pretrained(MODEL_ID, export=True, device=DEVICE, compile=False)\n",
    "    ov_pipe.save_pretrained(MODEL_DIR)\n",
    "else:\n",
    "    ov_pipe = OVStableDiffusionPipeline.from_pretrained(MODEL_DIR, device=DEVICE, compile=False)\n",
    "\n",
    "ov_pipe.compile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Image generation\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Now, you can define a text prompt for image generation and run inference pipeline.\n",
    "\n",
    "> **Note**: Consider increasing `steps` to get more precise results. A suggested value is `50`, but it will take longer time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = (\n",
    "    \"cyberpunk cityscape like Tokyo New York  with tall buildings at dusk golden hour cinematic lighting, epic composition. \"\n",
    "    \"A golden daylight, hyper-realistic environment. \"\n",
    "    \"Hyper and intricate detail, photo-realistic. \"\n",
    "    \"Cinematic and volumetric light. \"\n",
    "    \"Epic concept art. \"\n",
    "    \"Octane render and Unreal Engine, trending on artstation\"\n",
    ")\n",
    "text_prompt = widgets.Text(value=sample_text, description=\"your text\")\n",
    "num_steps = widgets.IntSlider(min=1, max=50, value=20, description=\"steps:\")\n",
    "seed = widgets.IntSlider(min=0, max=10000000, description=\"seed: \", value=42)\n",
    "widgets.VBox([text_prompt, num_steps, seed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pipeline settings\")\n",
    "print(f\"Input text: {text_prompt.value}\")\n",
    "print(f\"Seed: {seed.value}\")\n",
    "print(f\"Number of steps: {num_steps.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate an image and save the generation results.\n",
    "The pipeline returns one or several results: `images` contains final generated image. To get more than one result, you can set the `num_images_per_prompt` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(seed.value)\n",
    "\n",
    "result = ov_pipe(text_prompt.value, num_inference_steps=num_steps.value)\n",
    "\n",
    "final_image = result[\"images\"][0]\n",
    "final_image.save(\"result.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is show time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\\n\\t\".join(text_prompt.value.split(\".\"))\n",
    "print(\"Input text:\")\n",
    "print(\"\\t\" + text)\n",
    "display(final_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. As you can see, the picture has quite a high definition üî•."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image-to-Image pipeline\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "### Create image-to-Image pipeline\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "For running the Stable Diffusion model with Optimum Intel, we will use the `optimum.intel.OVStableDiffusionImg2ImgPipeline` class, which represents the inference pipeline. We will use the same model as for text-to-image pipeline. The model has already been downloaded from the Hugging Face Hub and converted to OpenVINO IR format on previous steps, so we can just load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVStableDiffusionImg2ImgPipeline\n",
    "from pathlib import Path\n",
    "\n",
    "DEVICE = device.value\n",
    "\n",
    "ov_pipe_i2i = OVStableDiffusionImg2ImgPipeline.from_pretrained(MODEL_DIR, device=DEVICE, compile=False)\n",
    "ov_pipe_i2i.compile()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image-to-Image generation\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Image-to-Image generation, additionally to text prompt, requires providing initial image. Optionally, you can also change `strength` parameter, which is a value between 0.0 and 1.0, that controls the amount of noise that is added to the input image. Values that approach 1.0 enable lots of variations but will also produce images that are not semantically consistent with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_prompt_i2i = widgets.Text(value=\"amazing watercolor painting\", description=\"Liberty Bell\")\n",
    "num_steps_i2i = widgets.IntSlider(min=1, max=50, value=10, description=\"steps:\")\n",
    "seed_i2i = widgets.IntSlider(min=0, max=1024, description=\"seed: \", value=42)\n",
    "image_widget = widgets.FileUpload(\n",
    "    accept=\"\",\n",
    "    multiple=False,\n",
    "    description=\"Upload image\",\n",
    ")\n",
    "strength = widgets.FloatSlider(min=0, max=1, description=\"strength: \", value=0.5)\n",
    "widgets.VBox([text_prompt_i2i, seed_i2i, num_steps_i2i, image_widget, strength])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "r = requests.get(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    ")\n",
    "\n",
    "open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import PIL\n",
    "\n",
    "#default_image_path = download_file(\n",
    "#    \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco.jpg\",\n",
    "#    filename=\"coco.jpg\",\n",
    "#)\n",
    "\n",
    "# read uploaded image\n",
    "image = PIL.Image.open(io.BytesIO(image_widget.value[-1][\"content\"]) if image_widget.value else str(default_image_path))\n",
    "print(\"Pipeline settings\")\n",
    "print(f\"Input text: {text_prompt_i2i.value}\")\n",
    "print(f\"Seed: {seed_i2i.value}\")\n",
    "print(f\"Number of steps: {num_steps_i2i.value}\")\n",
    "print(f\"Strength: {strength.value}\")\n",
    "print(\"Input image:\")\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scale_fit_to_window(dst_width: int, dst_height: int, image_width: int, image_height: int):\n",
    "    \"\"\"\n",
    "    Preprocessing helper function for calculating image size for resize with peserving original aspect ratio\n",
    "    and fitting image to specific window size\n",
    "\n",
    "    Parameters:\n",
    "      dst_width (int): destination window width\n",
    "      dst_height (int): destination window height\n",
    "      image_width (int): source image width\n",
    "      image_height (int): source image height\n",
    "    Returns:\n",
    "      result_width (int): calculated width for resize\n",
    "      result_height (int): calculated height for resize\n",
    "    \"\"\"\n",
    "    im_scale = min(dst_height / image_height, dst_width / image_width)\n",
    "    return int(im_scale * image_width), int(im_scale * image_height)\n",
    "\n",
    "\n",
    "def preprocess(image: PIL.Image.Image):\n",
    "    \"\"\"\n",
    "    Image preprocessing function. Takes image in PIL.Image format, resizes it to keep aspect ration and fits to model input window 512x512,\n",
    "    then converts it to np.ndarray and adds padding with zeros on right or bottom side of image (depends from aspect ratio), after that\n",
    "    converts data to float32 data type and change range of values from [0, 255] to [-1, 1].\n",
    "    The function returns preprocessed input tensor and padding size, which can be used in postprocessing.\n",
    "\n",
    "    Parameters:\n",
    "      image (PIL.Image.Image): input image\n",
    "    Returns:\n",
    "       image (np.ndarray): preprocessed image tensor\n",
    "       meta (Dict): dictionary with preprocessing metadata info\n",
    "    \"\"\"\n",
    "    src_width, src_height = image.size\n",
    "    dst_width, dst_height = scale_fit_to_window(512, 512, src_width, src_height)\n",
    "    image = np.array(image.resize((dst_width, dst_height), resample=PIL.Image.Resampling.LANCZOS))[None, :]\n",
    "    pad_width = 512 - dst_width\n",
    "    pad_height = 512 - dst_height\n",
    "    pad = ((0, 0), (0, pad_height), (0, pad_width), (0, 0))\n",
    "    image = np.pad(image, pad, mode=\"constant\")\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = 2.0 * image - 1.0\n",
    "    return image, {\"padding\": pad, \"src_width\": src_width, \"src_height\": src_height}\n",
    "\n",
    "\n",
    "def postprocess(image: PIL.Image.Image, orig_width: int, orig_height: int):\n",
    "    \"\"\"\n",
    "    Image postprocessing function. Takes image in PIL.Image format and metrics of original image. Image is cropped and resized to restore initial size.\n",
    "\n",
    "    Parameters:\n",
    "      image (PIL.Image.Image): input image\n",
    "      orig_width (int): original image width\n",
    "      orig_height (int): original image height\n",
    "    Returns:\n",
    "       image (PIL.Image.Image): postprocess image\n",
    "    \"\"\"\n",
    "    src_width, src_height = image.size\n",
    "    dst_width, dst_height = scale_fit_to_window(src_width, src_height, orig_width, orig_height)\n",
    "    image = image.crop((0, 0, dst_width, dst_height))\n",
    "    image = image.resize((orig_width, orig_height))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_image, meta_data = preprocess(image)\n",
    "\n",
    "np.random.seed(seed_i2i.value)\n",
    "\n",
    "processed_image = ov_pipe_i2i(text_prompt_i2i.value, preprocessed_image, num_inference_steps=num_steps_i2i.value, strength=strength.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image_i2i = postprocess(processed_image[\"images\"][0], meta_data[\"src_width\"], meta_data[\"src_height\"])\n",
    "final_image_i2i.save(\"result_i2i.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_i2i = \"\\n\\t\".join(text_prompt_i2i.value.split(\".\"))\n",
    "print(\"Input text:\")\n",
    "print(\"\\t\" + text_i2i)\n",
    "display(final_image_i2i)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Pytorch GPU",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/216524089-ed671fc7-a78b-42bf-aa96-9f7c791a9419.png",
   "tags": {
    "categories": [
     "Model Demos"
    ],
    "libraries": [],
    "other": [
     "Stable Diffusion"
    ],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "30f6166f5f0cb6253cad15b1c8ca46093b160f1914c051aeccf8063f98b299b9"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
